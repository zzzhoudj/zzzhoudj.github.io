<!DOCTYPE html>
<html lang="en"> 
<head>
    <title>DANet</title>
    
    <!-- Meta -->
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta name="description" content="Blog Template">
    <meta name="author" content="Xiaoying Riley at 3rd Wave Media">    
    <link rel="shortcut icon" href="favicon.ico"> 
    
    <!-- FontAwesome JS-->
    <script defer src="https://use.fontawesome.com/releases/v5.7.1/js/all.js" integrity="sha384-eVEQC9zshBn0rFj4+TU78eNA19HMNigMviK/PU/FFjLXqa/GKPgX58rvt5Z8PLs7" crossorigin="anonymous"></script>
    
    <!-- Plugin CSS -->
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.14.2/styles/monokai-sublime.min.css">
    
    <!-- Theme CSS -->  
    <link id="theme-style" rel="stylesheet" href="assets/css/theme-1.css">
    

</head> 

<body>
    
    <header class="header text-center">	    
	    <h1 class="blog-name pt-lg-4 mb-0"><a href="index.html">UCAS-ZDJ</a></h1>
        
	    <nav class="navbar navbar-expand-lg navbar-dark" >
           
			<button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navigation" aria-controls="navigation" aria-expanded="false" aria-label="Toggle navigation">
			<span class="navbar-toggler-icon"></span>
			</button>

			<div id="navigation" class="collapse navbar-collapse flex-column" >
				<div class="profile-section pt-3 pt-lg-0">
				    <img class="profile-image mb-3 rounded-circle mx-auto" src="assets/images/profile.png" alt="image" >			
					
					<div class="bio mb-2"><h5>周登继</h5></div>
					<div class="bio mb-3"><h5>中国科学院大学</h5></div>
					<!-- <ul class="social-list list-inline py-3 mx-auto">
			            <li class="list-inline-item"><a href="#"><i class="fab fa-twitter fa-fw"></i></a></li>
			            <li class="list-inline-item"><a href="#"><i class="fab fa-linkedin-in fa-fw"></i></a></li>
			            <li class="list-inline-item"><a href="#"><i class="fab fa-github-alt fa-fw"></i></a></li>
			            <li class="list-inline-item"><a href="#"><i class="fab fa-stack-overflow fa-fw"></i></a></li>
			            <li class="list-inline-item"><a href="#"><i class="fab fa-codepen fa-fw"></i></a></li>
					</ul> -->
					<a href="https://blog.csdn.net/qq_37935516" target="_blank" class="nav-item text-danger">CSDN</a>&nbsp;&nbsp;
					<a href="https://github.com/zzzhoudj" target="_blank" class="nav-item text-danger">GitHub</a>
			        <hr> 
				</div><!--//profile-section-->
				
				<ul class="navbar-nav flex-column text-left">
						<li class="nav-item active">
							<a class="nav-link" href="index.html"><i class="fas fa-home fa-fw mr-2"></i>博客主页 <span class="sr-only">(current)</span></a>
						</li>
						<li class="nav-item">
							<a class="nav-link" href="blog-list.html"><i class="fas fa-bookmark fa-fw mr-2"></i>博客列表</a>
						</li>
						<li class="nav-item">
							<a class="nav-link" href="about.html"><i class="fas fa-user fa-fw mr-2"></i>关于我</a>
						</li>
					</ul>
					
					<div class="my-2 my-md-3">
						<a class="btn btn-primary" href="mailto:zhouengji19@mails.ucas.edu.cn">邮件联系</a>
					</div>
			</div>
		</nav>
    </header>
    
    <div class="main-wrapper">
	    
	    <article class="blog-post px-3 py-5 p-md-5">
		    <div class="container">
			    <header class="blog-post-header">
				    <h2 class="title mb-2">【CVPR 2019】Dual Attention Network for Scene Segmentation</h2>
				    <div class="meta mb-3"><span class="date">Published: 2019-08-16</span><span class="time">10 min read</span></div>
			    </header>
			    
			    <div class="blog-post-body">
				    <!-- <figure class="blog-banner">
				        <a href="#"><img class="img-fluid" src="assets/images/blog/blog-post-banner.jpg" alt="image"></a>
				        <figcaption class="mt-2 text-center image-caption">Image Credit: <a href="#?ref=devblog" target="_blank">made4dev.com (Premium Programming T-shirts)</a></figcaption>
					</figure> -->
					<blockquote>
							<p><small>Attention机制在NLP上取得进展后于近年被引入CV中也发挥着重要的作用。这篇是采用Self-Attention机制的用于场景分割的文章，通过引入自注意力机制(self-attention mechanism) 在特征的空间维度和通道维度分别抓取特征之间的全局依赖关系，增强特征的表达能力，并在一些数据集上显著提升了性能。</small></p>
							<p><small>原文链接：<a href="https://arxiv.org/abs/1809.02983" target="_blank">https://arxiv.org/abs/1809.02983</a></small></p>
							<p><small>Github：<a href="https://github.com/junfu1115/DANet" target="_blank">https://github.com/junfu1115/DANet</a></small></p>
					</blockquote>
					<hr>
					<p>关于Self-Attention的解释，从网上找了很多self-attention的文章，发现大多是NLP的，CV的文章寥寥无几。下面是找到的一些self-attention in cv的文章：</p>
					<ul>
						<li><a href="https://www.jianshu.com/p/a9771abedf50" target="_blank">https://www.jianshu.com/p/a9771abedf50</a></li>
						<li><a href="https://www.jianshu.com/p/8f5c13aa19a8" target="_blank">https://www.jianshu.com/p/8f5c13aa19a8</a></li>
						<li><a href="https://cloud.tencent.com/developer/news/247227" target="_blank">https://cloud.tencent.com/developer/news/247227</a></li>
						<li><a href="https://www.jiqizhixin.com/articles/2019-02-15-12" target="_blank">https://www.jiqizhixin.com/articles/2019-02-15-12</a></li>
					</ul>
					<h3 class="mt-5 mb-3">Abstract</h3>
				    <p>本文中我们通过捕获基于自注意机制的丰富上下文相关性来解决场景分割任务。与以往多尺度特征融合捕获的上下文不同的是，我们提出了一种双重注意力机制网络（DANet）以自适应地将局部特征与其全局依赖集成起来。具体来说就是在扩展的FCN顶部增加了两个注意力模块，分别对空间维度和通道维度上的语义相互依赖进行建模。Position attention module通过对所有位置上的特征加权加权求和选择性的集成特征。相似的特征能够被互相关联起来，而与距离无关。同时，channel attention module通过在所有通道映射之间集成相关联的特征来选择性地强调相互依赖的通道映射。我们对两个模块的输出求和以提升特征的表达能力，从而获得更准确的分割结果。我们在Cityscapes、PASCAL Context和COCO三大数据集上取得了先进的分割效果。特别是在不使用粗Cityscapes数据的情况下，在测试集上获得81.5%的平均IoU。</p>
					
					<h3 class="mt-5 mb-3">Introduction</h3>
				    <p>我们提出了DANet用于场景分割，它引入自注意力机制来捕获依赖于空间维度和通道维度的特征。特别的，我们在扩展FCN上追加了两个平行的注意力模块，一个称为position attention module，另一个称为channel attention module。对于position attention module来说，采用self-attention机制能够捕获任意两个位置的特征图之间的空间依赖关系。对于channel attention module来说，我们采用类似于self-attention的机制捕获任意两个通道映射之间的通道依赖关系，并对所有的通道映射的加权和来更新每个通道映射。最后，融合两个模块的输出以进一步增强特征表示。</p>
					<p>我们的放在在处理复杂和多样性场景的时候比以往方法更有效和灵活。本文的贡献主要有一下三点：</p>
					<ul>
						<li>采用self-attention 机制提出一种双注意力机制网络DANet来增强特征表示在场景分割中的鉴别能力；</li>
						<li>提出一种位置注意力模块用来学习特征之间的空间依赖关系和通道注意力模块用来学习通道之间的依赖关系，通过局部特征丰富的上下文建模显著提升了分割效果；</li>
						<li>在三大数据集上达到了先进效果。</li>
					</ul>
					<figure class="blog-banner">
							<a href="assets/images/blog/DANet/DANet-thumb.jpg"><img class="img-fluid" src="assets/images/blog/DANet/DANet-thumb.jpg" alt="image"></a>
							<figcaption class="mt-2 text-center image-caption">DANet结构图</figcaption>
					</figure>

					<h3 class="mt-5 mb-3">Dual Attention Network</h3>
					<p>在本节中介绍了网络的总体框架，然后介绍了在空间维度和通道维度上分别捕获远程上下文信息的注意模块。最后，我们将描述如何将它们聚合在一起以进一步细化。</p>
					<h4 class="mt-5 mb-3">Overview</h4>
					<p>由于卷积操作使用局部感受野进行特征提取，相同类别的物体可能会有不同的特征。<b>这些不同导致了类内差异的不一致性并对识别的准确率造成影响。</b>为了解决这个问题，我们通过注意力机制构造了特征之间的关系探索了全局上下文信息。</p>
					<p>文中使用空洞ResNet作为主体网络生成局部特征，但是取消了最后两块的downsampling操作，采用空洞卷积来达到既扩大感受野又保持较高空间分辨率的目的。</p>
					<p>下图是position attention module和channel attention module的具体结构:</p>
					<figure class="blog-banner"  style="text-align:center">
							<a href="assets/images/blog/DANet/DANet-attention.jpg"><img class="img-fluid" src="assets/images/blog/DANet/DANet-attention.jpg" alt="image"></a>
							<figcaption class="mt-2 text-center image-caption">Position attention module & Channel attention module</figcaption>
					</figure>
					<h4 class="mt-5 mb-3">Position Attention Module</h4>
					<p>输入A是C*H*W，经过卷积之后生成B、C、D（C*H*W），然后将B/C调整为C*N（N=H*W），将B转置（B=N*C）然后与C做matrix multiplication，然后应用softmax计算空间注意力映射S（N*N），matrix multiplication公式如下：</p>
					<figure class="blog-banner" style="text-align:center">
							<a href="assets/images/blog/DANet/DANet-matrix-multiplication.jpg"><img class="img-fluid" src="assets/images/blog/DANet/DANet-matrix-multiplication.jpg" alt="image"></a>
					</figure>
					<p>将D也调整为C*N，与S的转置N*N做matrix multiplication，并将result调整为C*H*W，最后与A做element-wise sum操作，得到输出E（C*H*W），element-wise sum公式如下：</p>
					<figure class="blog-banner"  style="text-align:center">
							<a href="assets/images/blog/DANet/DANet-element-wise-sum.jpg"><img class="img-fluid" src="assets/images/blog/DANet/DANet-element-wise-sum.jpg" alt="image"></a>
					</figure>
					<h4 class="mt-5 mb-3">Channel Attention Module</h4>
					<p>与position attention module类似，不同的是通道注意映射X是C*C大小的。</p>
					<p>在两个注意力模块输出之后采用element-wise sum进行融合，然后卷积得到最终预测图。</p>


					<h3 class="mt-5 mb-3">Result</h3>
					<p>下图所示可能看到DANet在Cityscapes数据集上比以往的方法有明显的优势:</p>
					<figure class="blog-banner">
							<a href="assets/images/blog/DANet/DANet-result-table.png"><img class="img-fluid" src="assets/images/blog/DANet/DANet-result-table.png" alt="image"></a>
							<figcaption class="mt-2 text-center image-caption">DANet与其他方法的比较</figcaption>
					</figure>
					<figure class="blog-banner">
							<a href="assets/images/blog/DANet/DANet-result.png"><img class="img-fluid" src="assets/images/blog/DANet/DANet-result.png" alt="image"></a>
							<figcaption class="mt-2 text-center image-caption">DANet的分割效果和attention map</figcaption>
					</figure>
					<p><small>转载请联系：<a href="mailto:zhoudengji19@mails.ucas.edu.cn">zhoudengji19@mails.ucas.edu.cn</a></small></p>

				   
			    </div>
				    
			    <nav class="blog-nav nav nav-justified my-5">
				  <a class="nav-link-prev nav-item nav-link rounded-left" href="CCNet.html">Previous<i class="arrow-prev fas fa-long-arrow-alt-left"></i></a>
				  <a class="nav-link-next nav-item nav-link rounded-right" href="Gated-SCNN.html">Next<i class="arrow-next fas fa-long-arrow-alt-right"></i></a>
				</nav>
				
		    </div><!--//container-->
		</article>
		    
	    <footer class="footer text-center py-2 theme-bg-dark">
				<small class="copyright">&copy 2019 • ZhouDengji</small>
	    </footer>
    
    </div><!--//main-wrapper-->
    

    <!-- *****CONFIGURE STYLE (REMOVE ON YOUR PRODUCTION SITE)****** -->  
    <div id="config-panel" class="config-panel d-none d-lg-block">
        <div class="panel-inner">
            <a id="config-trigger" class="config-trigger config-panel-hide text-center" href="#"><i class="fas fa-cog fa-spin mx-auto" data-fa-transform="down-6" ></i></a>
            <h5 class="panel-title">Choose Colour</h5>
            <ul id="color-options" class="list-inline mb-0">
                <li class="theme-1 active list-inline-item"><a data-style="assets/css/theme-1.css" href="#"></a></li>
                <li class="theme-2  list-inline-item"><a data-style="assets/css/theme-2.css" href="#"></a></li>
                <li class="theme-3  list-inline-item"><a data-style="assets/css/theme-3.css" href="#"></a></li>
                <li class="theme-4  list-inline-item"><a data-style="assets/css/theme-4.css" href="#"></a></li>
                <li class="theme-5  list-inline-item"><a data-style="assets/css/theme-5.css" href="#"></a></li>
                <li class="theme-6  list-inline-item"><a data-style="assets/css/theme-6.css" href="#"></a></li>
                <li class="theme-7  list-inline-item"><a data-style="assets/css/theme-7.css" href="#"></a></li>
                <li class="theme-8  list-inline-item"><a data-style="assets/css/theme-8.css" href="#"></a></li>
            </ul>
            <a id="config-close" class="close" href="#"><i class="fa fa-times-circle"></i></a>
        </div><!--//panel-inner-->
    </div><!--//configure-panel-->

    
       
    <!-- Javascript -->          
    <script src="assets/plugins/jquery-3.3.1.min.js"></script>
    <script src="assets/plugins/popper.min.js"></script> 
    <script src="assets/plugins/bootstrap/js/bootstrap.min.js"></script> 
    
    <!-- Page Specific JS -->
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.14.2/highlight.min.js"></script>

    <!-- Custom JS -->
    <script src="assets/js/blog.js"></script>
    
    <!-- Style Switcher (REMOVE ON YOUR PRODUCTION SITE) -->
    <script src="assets/js/demo/style-switcher.js"></script>     
    

</body>
</html> 

